# Example of ResNet

- [config.json in use](##config.json in use)
- [Step by step](##Step by step)



ResNet case uses a binary classification task as an example. The `features` data` little_exp.tsv` contains 100 samples and 23361 features.

```bash
> wc little_exp.tsv | awk '{print $1}'  # rows
100
> sed -n '1,1p' little_exp.tsv | awk '{print NF}'  # columns
23361
```

`target` data are of two types, 70 and 30 respectively.

```bash
> grep -c 0 little_learning_target.tsv
70
> grep -c 1 little_learning_target.tsv
30
```



First, prepare the configuration file, as shown below:

## config.json  in use

```javascript
{
    "name": "resnet_demo",  // Project name

    "model": {
        "type": "ResNet",
        "args": {
            "output_classes": 2}  // Number of classes
    },

    "data_train": {
        "type": "DataLoader",
        "args":{
            "data_dir": "/data_autogenome/data_test",  //The folder where the data is located, it is recommended to use an absolute path
            "features_file": "little_exp.tsv",   //File of features
            "labels_file": "little_learning_target.tsv",   //File of target
            "validation_split": 0.2,  //Validation set ratio
            "shuffle": true,  //Whether to shuffle the data during training
            "delimiter": " "  //Delimiter in data, default: '\t' 
        }
    },

    "data_evaluate": {
        "type": "DataLoader",
        "args": {
            "data_dir": "/data_autogenome/data_test",
            "features_file": "little_exp.tsv",  //File for evaluation
            "labels_file": "little_learning_target.tsv",  //target for evaluation
            "delimiter": " "
        }
    },

    "data_predict": {
        "type": "DataLoader",
        "args": {
            "data_dir": "/data_autogenome/data_test",
            "features_file": "little_exp.tsv",  //File for prediction
            "delimiter": " "
        }
    },

    "input_fn": {
        "num_threads": 16  //Number of threads for reading data
    },

    "trainer": {
        "hyper_selector": true,  //[true/false]，Whether to perform hyperparameter search
        "batch_size": 64,  
        "save_dir": "./experiments",  //Folder where logs, model parameters, and results are saved
        "monitor": "accuracy",  //[accuracy/loss/f1_score]，Evaluate metrics on the validation set during training
        "num_gpus": 1, 
        "patience": 30,   //After the number of consecutive epochs do not improve the performance on the validation set, stop training
        "pre_train_epoch": 10,  //The number of epochs trained in each group of parameters in Hyperparameter search phase
        "max_steps": 64000,  //Maximum running steps to train. When the patience is large, the max_steps steps will be trained. Max_steps * batch_size / num_samples is the number of epochs corresponding to the training.
        "loss": "cross_entropy",
        "selected_mode": "max"   //[max/min]Bigger or smaller is better when evaluation
    },

    "optimizer": {
        "type": "adam"  //[adam/sgd/adadelta/adagrad/adamw/ftrl/momentum/rmsprop/kfac/dynamic_momentum/lamb]
    },

    "evaluator": {
        "max_number_of_steps": 10,
        "batch_size":100
    },

    "predictor": {
        "log_every_n_steps": 10,
        "output_every_n_steps": 1,
        "max_number_of_steps": 100,
        "batch_size":100
    },

    "explainer": {
        "args": {
            "features_name_file": "/data_autogenome/data_test/names_2.txt",  //File of feature name ，one column，rows are equall to the number of features
            "plot_type": "bar",  //[bar/dot/violin/layered_violin]
            "num_samples": 80,  //Number of sample used in explainer
            "ranked_outputs": 20  //Number of important variables will be ploted
        }
    },

    "param_spec": {   //Initial parameters
        "type": "origin_params",
        "args": {
            "ResNet": {
                "n_latent": 32,
                "n_blocks": 2,
                "keep_prob": 0.8,
                "output_nonlinear": "sigmoid"
            },
            "optimizer_param": {
                "learning_rate": 0.0001
            }
        }
    },

    "hyper_param_spec": {    //Hyperparameter search space
        "type": "hyper_params",
        "args": {
            "ResNet": {
                "n_latent": [128, 64, 32],
                "n_blocks": [2, 3],
                "keep_prob": [0.8, 1.0],
                "output_nonlinear": [null, "relu", "tanh", "sigmoid"]
            },
            "optimizer_param": {

            }
        }
}
}

```



## Step by step

The use of AutoGenome mainly includes the following steps:

1. import autogenome

   ```python
   import autogenome as ag
   ```

2. Read the configuration file, as shown above

   ```python
   automl = ag.auto("/data_autogenome/data_test/json_hub_simple/resnet_test.json")
   ```

   After the configuration file is successfully read, the following log is printed:

   ```javascript
   ==========================================================
   ----------Use the config from /data_autogenome/data_test/json_hub_simple/resnet_test.json
   ----------Initialize data_loader class
   ----------Initialize Optimizer
   ----------Initialize hyper_param_spec class 
   ----------Initialize param_spec class 
   ==========================================================
   #################################################################################
   #                                                                               #
   #                                                                               #
            Ready to search the best hyper parameters for ResNet model            
   #                                                                               #
   #                                                                               #
   #################################################################################
   ```

   

3. Train the model. Model training is performed according to the training parameters in the configuration file, and the data set is divided into training sets: validation set in ratio of 8: 2. the training set data is used for training, and the validation set data is evaluated at the same time. Better models and parameters are saved to the `models` folder in the corresponding folder (trainer.saver:" ./experiments ")

   ```python
   automl.train()
   ```

   During training, the model is initialized to the parameters in `param_spec`, and then the parameter search is performed in the hyperparameter search space` hyper_param_spec`. Each parameter combination will train a certain number of epochs (trainer.pre_train_epoch). After hyperparameter search final model structure parameters  is sured. The final model will be trained, when the results are better, the model parameters are saved, and it will stop train when the effect on the validation set is not improved after several consecutive epochs (trainer.patience).

   Some logs during training are as follows:

   ```javascript
   ----------In Hyper & Training Search stage 
   ...
   Pretraining: search parameter is n_latent, search value is 32
   LR_RANGE_TEST From begin_lr 0.000010 To end_lr 0.100000, 
   Graph was finalized.
   Running local_init_op.
   Done running local_init_op.
   Running will end at step: 10
   step: 0(global step: 0)	sample/sec: 141.683	loss: 0.674	accuracy: 0.609
   ...
   [Plateau Metric] step: 163 loss: 1.213 accuracy: 0.828
   [Plateau Metric] step: 165 loss: 0.726 accuracy: 0.906
   Saving checkpoints for 166 into experiments/models/resnet_demo/1218_160039/hyper_lr0.08500150000000001_keep_prob_0.8_n_latent_128_output_nonlinear_None_n_blocks_2/best_model.ckpt.
   [Plateau Metric] step: 167 loss: 1.827 accuracy: 0.828
   [Plateau Metric] step: 169 loss: 1.776 accuracy: 0.828
   [Plateau Metric] step: 171 loss: 0.910 accuracy: 0.891
   [Plateau Metric] step: 173 loss: 0.207 accuracy: 0.984
   Saving checkpoints for 174 into experiments/models/resnet_demo/1218_160039/hyper_lr0.08500150000000001_keep_prob_0.8_n_latent_128_output_nonlinear_None_n_blocks_2/best_model.ckpt.
   [Plateau Metric] step: 175 loss: 0.181 accuracy: 0.984
   ...
   ```

   

4. Evaluation. According to the model structure and model parameters trained in the previous step, evaluate the performance on `data_evaluate`. The classification model will print the accuracy value and the confusion matrix.

   ```python
   automl.evaluate()
   ```

   Some logs are as follows:

   ```javascript
   ----------In Evaluation stage 
   Restoring parameters from experiments/models/resnet_demo/1218_160039/hyper_lr0.08500150000000001_keep_prob_0.8_n_latent_128_output_nonlinear_None_n_blocks_2/best_model.ckpt-200
   Running local_init_op.
   Done running local_init_op.
   step: 1	batch/sec: 117.715	loss: 0.007	accuracy: 1.000
   step: 3	batch/sec: 133.102	loss: 0.007	accuracy: 1.000
   step: 5	batch/sec: 115.314	loss: 0.007	accuracy: 1.000
   step: 7	batch/sec: 103.886	loss: 0.007	accuracy: 1.000
   step: 9	batch/sec: 99.960	loss: 0.007	accuracy: 1.000
   ...
   loss: 0.006915087699890137	accuracy: 1.0		[100 batches]
   Confusion matrix plot is 'experiments/output_files/resnet_demo/1218_160039/ResNet_confusion_matrix.pdf'
   ```

   The resulting confusion matrix is shown below. The size of the figure will be adjusted according to the number of categories. The x-axis is true value and the y-axis is the predicted value.

   ![](../images/resnet_confusion_matrix.png)

   

5. Prediction. According to the model structure and model parameters trained in step 3, for a given features data, the classification model predicts its category and the softmax value of each category, and will be saved as csv file.

   ```python
   automl.predict()
   ```

   Some logs are as follows:

   ```javascript
   ----------In Prediction stage 
   Graph was finalized.
   Restoring parameters from experiments/models/resnet_demo/1218_160039/hyper_lr0.08500150000000001_keep_prob_0.8_n_latent_128_output_nonlinear_None_n_blocks_2/best_model.ckpt-200
   Running local_init_op.
   Done running local_init_op.
   step: 9	batch/sec: 110.194
   	[10 batches]
   Predicted values file is "experiments/output_files/resnet_demo/1218_160039/ResNet_predicted_result_data_frame.csv"
   ```

   As shown in the log, the predicted target file will be produced. The first column of the file is `predicted_result`, and the second column of `softmax_value` is the softmax value of each category.

6. Explanation. Sort the importance of the variables according to the model trained in step 3. The file corresponding to the variable name needs to be specified in the `explainer`.

   ```python
   automl.explain()
   ```

   Sort the importance of the model variables, and the log is as follows:

   ```javascript
   ----------Initialize Shap class
   Restoring parameters from experiments/models/resnet_demo/1218_160039/hyper_lr0.08500150000000001_keep_prob_0.8_n_latent_128_output_nonlinear_None_n_blocks_2/best_model.ckpt-200
   ----------Computing shap_values with 80  examples and 23361 features
   importance plot is 'experiments/output_files/resnet_demo/1218_160039/_dot0_feature_importance_summary.pdf'
   importance plot is 'experiments/output_files/resnet_demo/1218_160039/_dot1_feature_importance_summary.pdf'
   features orders in all classes is saved in 'experiments/output_files/resnet_demo/1218_160039/_features_orders.csv'
   importance plot for every classes is 'experiments/output_files/resnet_demo/1218_160039/class_1feature_importance_summary.pdf'
   importance plot for every classes is 'experiments/output_files/resnet_demo/1218_160039/class_0feature_importance_summary.pdf'
   importance plot is 'experiments/output_files/resnet_demo/1218_160039/_barTotal_feature_importance_summary.pdf'
   shap_values every classes is 'experiments/output_files/resnet_demo/1218_160039/_class_0shap_values.csv'
   shap_values every classes is 'experiments/output_files/resnet_demo/1218_160039/_class_1shap_values.csv
   ```

   Variable importance bar charts and dot charts and total variable importance charts for each category are produced, as shown below:

   Importance map for all variable：
   ![](/images/resnet_total_feature_importance_summary.png)

   Features importance bar plot for class1:
   ![](/images/resnet_class1_feature_importance_summary_bar.png)
   
   Features importance bar plot for class0:
   ![](/images/resnet_class0_feature_importance_summary_bar.png)
   
   class 1变量重要性点图：Features importance dot plot for class1:
   ![](/images/resnet_class1_feature_importance_summary_dot.png)
   
   Features importance dot plot for class0:
   ![](/images/resnet_class0_feature_importance_summary_dot.png)
