# MLP案例

- [配置文件](##配置文件)
- [使用](##使用)



MLP案例以一个二分类任务为例，`features`数据`little_exp.tsv`包含100个samples，23361个features。

```bash
> wc little_exp.tsv | awk '{print $1}'  # 打印行数
100
> sed -n '1,1p' little_exp.tsv | awk '{print NF}'  # 打印列数
23361
```

`target`数据为两类，分别为70和30。

```bash
> grep -c 0 little_learning_target.tsv
70
> grep -c 1 little_learning_target.tsv
30
```



首先，准备配置文件，具体如下所示：

```javascript
{
    "name": "mlp_demo",   // Project name

    "model": {
        "type": "MLP",
        "args": {
            "output_classes": 2}   // 分类数目
    },

    "data_train": {
        "type": "DataLoader",
        "args":{
            "data_dir": "/data_autogenome/data_test",   //数据所在文件夹，推荐使用绝对路径
            "features_file": "little_exp.tsv",  //features文件名
            "labels_file": "little_learning_target.tsv",   //target文件名
            "validation_split": 0.2,   //验证集比例
            "shuffle": true,   //训练时是否对数据进行打乱操作
            "delimiter": " "   //数据分隔符，default: '\t'
        }
    },

    "data_evaluate": {
        "type": "DataLoader",
        "args": {
            "data_dir": "./data_autogenome/data_test",
            "features_file": "little_exp.tsv",   //用于单独评估的features文件名
            "labels_file": "little_learning_target.tsv",  //用于单独评估的target文件名
            "delimiter": " "
        }
    },

    "data_predict": {
        "type": "DataLoader",
        "args": {
            "data_dir": "./data_autogenome/data_test", 
            "features_file": "little_exp.tsv",   //用于单独预测的features文件名
            "delimiter": " "
        }
    },

    "input_fn": {
        "num_threads": 16   //读取数据线程数
    },

    "trainer": {
        "hyper_selector": true,   //[true/false]，是否进行超参搜索
        "batch_size": 64,   //每次训练的样本数大小
        "save_dir": "./experiments",   //保存日志、模型参数、结果输出的文件夹名称
        "monitor": "accuracy",   //[accuracy/loss/f1_score]，训练过程中在验证集上评估指标
        "num_gpus": 1,   //机器可使用gpu个数
        "patience": 20,   //连续多少个epoch后在验证集上效果没有再提升，则停止训练
        "pre_train_epoch": 10,   //超参搜索阶段，每组参数训练的epoch数目
        "max_steps": 64000,   //训练最大运行的steps，当patience很大时则会训练max_steps步，max_steps*batch_size/num_samples为对应训练的epoch数
        "loss": "cross_entropy",
        "selected_mode": "max"   //[max/min]验证集上评估指标是越大越好还是越小越好
    },

    "optimizer": {
        "type": "adam"    //[adam/sgd/adadelta/adagrad/adamw/ftrl/momentum/rmsprop/kfac/dynamic_momentum/lamb]
    },

    "evaluator": {
        "max_number_of_steps": 1,
        "batch_size":100

    },

    "predictor": {
        "max_number_of_steps": 1,
        "batch_size":100
    },

    "explainer": {
        "type": "Shap",
        "args": {
            "plot_type": "bar",  //[bar/dot/violin/layered_violin]
            "features_name_file": "/data_autogenome/data_test/names_2.txt",  //变量名文件，一列，行数为变量个数
            "num_samples": 80,   //使用的样本数
            "ranked_outputs": 20  //输出变量重要性前多少个
        }
    },

    "param_spec": {   //模型初始参数，不进行超参搜索时则直接训练该模型
        "type": "origin_params",
        "args": {
            "MLP": {
                "layers_list": [64， 128],  //网络结构
                "keep_prob": 1,
                "output_nonlinear": "sigmoid"
            },
            "optimizer_param": {
                "learning_rate": 0.0001
            }
        }
    },

    "hyper_param_spec": {  //超参搜索空间
        "type": "hyper_params",
        "args": {
            "MLP": {
                "size_candidates": [128, 64],   //每层网络节点搜索空间
                "num_layers": [2, 3],  //隐藏网络层数
                "keep_prob": [0.8, 1.0],
                "output_nonlinear": [null, "relu", "tanh", "sigmoid"]
            },
            "optimizer_param": {

            }
        }
}
}

```



## 使用

AutoGenome的使用主要包含以下几步：

1. 导入autogenome包

   ```python
   import autogenome as ag
   ```

2. 读取配置文件，配置文件如上述所示

   ```python
   automl = ag.auto("/data_autogenome/data_test/json_hub_simple/densenet_test.json")
   ```

   配置文件读取成功后，会打印如下日志：

   ```javascript
   ==========================================================
   ----------Use the config from 
   /data_autogenome/data_test/json_hub_simple/mlp_test.json
   ----------Initialize data_loader class
   ----------Initialize Optimizer
   ----------Initialize hyper_param_spec class 
   ----------Initialize param_spec class 
   ==========================================================
   #################################################################################
   #                                                                               #
   #                                                                               #
            Ready to search the best hyper parameters for MLP model            
   #                                                                               #
   #                                                                               #
   #################################################################################
   ```

   

3. 训练模型。根据配置文件中训练参数进行模型训练，将数据集划分为训练集:验证集为8:2，使用训练集数据进行训练，同时在验证集数据上进行评估，保存在验证集数据上评价指标（trainer.monitor: "accuracy"）更好的模型和参数到相应文件夹（trainer.saver: "./experiments"）中的`models`文件夹中

   ```python
   automl.train()
   ```

   训练过程中，先将模型初始化为`param_spec`中参数，然后在超参搜索空间`hyper_param_spec`中进行参数搜索，每一种参数组合会训练一定的epoch数（trainer.pre_train_epoch），挑选结果最好的作为最终模型结构参数并进行进一步训练，训练过程中在验证集上进行评估，当结果更好时保存模型参数，在连续数个epoch（trainer.patience）后在验证集上效果没有再提升，则停止训练。

   训练过程中部分日志如下:

   ```javascript
   ----------In Hyper & Training Search stage
   ...
   Pretraining: search parameter is layers_list, search value is (128, 64)
   LR_RANGE_TEST From begin_lr 0.000010 To end_lr 0.100000, 
   Graph was finalized.
   Running local_init_op.
   Done running local_init_op.
   Running will end at step: 10
   step: 0(global step: 0)	sample/sec: 218.444	loss: 0.805	accuracy: 0.484
   ...
   The model and events are saved in experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None
   Graph was finalized.
   Running local_init_op.
   Done running local_init_op.
   Running will end at step: 64000
   step: 0(global step: 0)	sample/sec: 86.607	loss: 0.843	accuracy: 0.531
   [Plateau Metric] step: 1 loss: 147.601 accuracy: 0.266
   Saving checkpoints for 2 into experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None/best_model.ckpt.
   [Plateau Metric] step: 3 loss: 64.228 accuracy: 0.266
   [Plateau Metric] step: 5 loss: 67.668 accuracy: 0.328
   Saving checkpoints for 6 into experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None/best_model.ckpt.
   [Plateau Metric] step: 7 loss: 122.216 accuracy: 0.328
   ...
   step: 200(global step: 200)	sample/sec: 4496.934	loss: 0.000	accuracy: 1.000
   [Plateau Metric] step: 201 loss: 0.046 accuracy: 0.953
   [Plateau Metric] step: 203 loss: 0.034 accuracy: 0.953
   [Plateau Metric] step: 205 loss: 0.025 accuracy: 1.000
   Saving checkpoints for 206 into experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None/best_model.ckpt.
   [Plateau Metric] step: 207 loss: 0.036 accuracy: 1.000
   ...
   ```

   

4. 评估模型。根据上步所训练的模型结构及模型参数，评估在`data_evaluate`上表现，分类模型输出`accuracy`值和`confusion matrix`

   ```python
   automl.evaluate()
   ```

   部分日志如下所示：

   ```javascript
   ----------In Evaluation stage
   Restoring parameters from experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None/best_model.ckpt-206
   Running local_init_op.
   Done running local_init_op.
   loss: 0.005278169643133879	accuracy: 1.0		[1 batches]
   Confusion matrix plot is 'experiments/output_files/mlp_demo/1225_113835/MLP_confusion_matrix.pdf'
   ```
   
   得到的confusion matrix图如下所示，图数字大小会根据类别数多少进行调整，x轴为真值，y轴为预测值：
   
   ![](../images/mlp_confusion_matrix.png)
   

   
5. 预测数据。根据第3步所训练的模型结构及模型参数，对于给定features数据，分类问题预测其类别及各个类别的softmax值，并输出到对应的csv文件中

   ```python
   automl.predict()
   ```

   日志如下所示：

   ```javascript
   ----------In Prediction stage 
   Restoring parameters from experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None/best_model.ckpt-206
   Running local_init_op.
   Done running local_init_op.
   	[1 batches]
   Predicted values file is "experiments/output_files/mlp_demo/1225_113835/MLP_predicted_result_data_frame.csv"
   ```

   如日志所示，将输出预测的target文件，文件第一列`predicted_result`，第二列`softmax_value`为各个类别的softmax值。

6. 变量重要性排序。根据第3步所训练的模型结构及模型参数，对变量重要性进行排序，输出各个类别变量重要性，`explainer`中需要指定变量名对应的文件。

   ```python
   automl.explain()
   ```

   对模型变量重要性进行排序，输出日志如下：

   ```javascript
   ----------Initialize Shap class
   Restoring parameters from experiments/models/mlp_demo/1225_113835/hyper_lr0.08500150000000001_layers_list_(128128)_keep_prob_0.8_output_nonlinear_None/best_model.ckpt-206
   ----------Computing shap_values with 80  examples and 23361 features
   importance plot is 'experiments/output_files/mlp_demo/1225_113835/_dot0_feature_importance_summary.pdf'
   importance plot is 'experiments/output_files/mlp_demo/1225_113835/_dot1_feature_importance_summary.pdf'
   features orders in all classes is saved in 'experiments/output_files/mlp_demo/1225_113835/_features_orders.csv'
   importance plot for every classes is 'experiments/output_files/mlp_demo/1225_113835/class_0feature_importance_summary.pdf'
   importance plot for every classes is 'experiments/output_files/mlp_demo/1225_113835/class_1feature_importance_summary.pdf'
   importance plot is 'experiments/output_files/mlp_demo/1225_113835/_barTotal_feature_importance_summary.pdf'
   shap_values every classes is 'experiments/output_files/mlp_demo/1225_113835/_class_0shap_values.csv'
   shap_values every classes is 'experiments/output_files/mlp_demo/1225_113835/_class_1shap_values.csv'
   ```

   运行结束后将输出各个类别的变量重要性条图和点图和总的变量重要性图，如下所示：

   总的变量重要性图：

   ![](../images/mlp_total_feature_importance_summary.png)

   class 1变量重要性条图：

   ![](../images/mlp_class1_feature_importance_summary_bar.png)

   class 0变量重要性条图：

   ![](../images/mlp_class0_feature_importance_summary_bar.png)

   class 1变量重要性点图：

   ![](../images/mlp_class1_feature_importance_summary_dot.png)

   class 0变量重要性点图：
   ![](../images/mlp_class0_feature_importance_summary_dot.png)
